---
title: "Untitled"
author: "Sandra Dela Cruz, Yanyi Li"
date: "2025-05-06"
output: html_document
---
```{r, include=FALSE}
# Load required packages
library(dplyr)
library(janitor)
library(stringr)
library(ggplot2)
library(tidyr)
library(purrr)
library(DataExplorer)
library(reshape2)
library(pROC) # for ROC/AUC
library(readr)
library(caret) # for confusion matrix
library(DHARMa)
library(lares)
library(mice)
library(gridExtra)
library(nnet) # for multinomial model
library(car) # check for multicollinearity
library(MLmetrics)
library(randomForest)
library(knitr)
library(kableExtra)
```

# **Data Exploration**
### **Initial Cleaning Before Splitting**
1. Changed column names to snake format
2. Add number column for row numbers before splitting to ensure no duplicate column is included in the evaluation data
3. Assign class as target variable
4. Rename index to gender
5. Observe data types and detect missing values using summary, no missing values detected
6. Correct variables data types

```{r, echo=FALSE}
library(readr)
dataset_of_diabetes <- read_csv("https://raw.githubusercontent.com/lalaexplore/DATA-621/refs/heads/main/Final%20Project/dataset_of_diabetes.csv")

# 1. Changed column names to snake format
# 2. Add number column for row numbers before splitting
# 3. Assign class as target variable
# "N" (No diabetes)
# "P" (Possibly at risk for diabetes)
# "Y" (Yes, diagnosed with diabetes)
dataset_of_diabetes <- dataset_of_diabetes |>
  mutate(number = row_number()) |>
  clean_names(case = "snake") |>
  rename(target = class, index = 3) |>
  rename(gender = index) |>
  select(id, no_pation, target, everything())

# 5. Observe data types and detect missing values using summary, no missing values detected
summary(dataset_of_diabetes)

# 6. Correct variables data types
dataset_of_diabetes <- dataset_of_diabetes |>
  mutate(
    gender = as.factor(gsub("^f$", "F", gender)),  # Replace lowercase 'f' with 'F'
    target = factor(
      target,
      levels = c("N", "P", "Y")  # No need to make it ordered unless there's a natural order
    )
  )
# Show corrected variables
str(dataset_of_diabetes)

plot_histogram(dataset_of_diabetes)
plot_qq(dataset_of_diabetes)
```

### **Split Dataset into Training and Evaluation**
1. Split original dataset into 8:2
2. Check the distribution of split, output okay
3. Identified overlapping number, none identified

```{r, echo=FALSE}
# using caret package
set.seed(621)
train_target <- createDataPartition(
  dataset_of_diabetes$target, p = 0.8, list = FALSE)
dataset_of_diabetes_training <- dataset_of_diabetes[train_target, ]
dataset_of_diabetes_evaluation <- dataset_of_diabetes[-train_target, ]

# check to ensure the distribution between the split
ggplot(dataset_of_diabetes, aes(x = target, fill = "All")) +
  geom_density(alpha = 0.3) +
  geom_density(data = dataset_of_diabetes_training, aes(
    x = target, fill = "Train"), alpha = 0.5) +
  geom_density(data = dataset_of_diabetes_evaluation, aes(
    x = target, fill = "Evaluation"), alpha = 0.5) +
  labs(title = "Distribution of Target in Train vs Evaluation") +
  theme_minimal()

# Identify overlapping indices
duplicates <- intersect(
  dataset_of_diabetes_training$number, 
  dataset_of_diabetes_evaluation$number)

# Output how many duplicates and optionally which ones
if(length(duplicates) > 0) {
  cat("Found", length(duplicates), "duplicate no_pation:\n")
  print(duplicates)
} else {
  cat("âœ… No duplicate indices between training and evaluation sets.\n")
}
```

### **Exploration of Both Datasets**
1. Remove id, no_pation, and number columns because we don't need it
2. Have an overview of both datasets
3. Prior to splitting the datasets into training and evaluation we already performed missing value evaluation and we also corrected the datatypes, and we see that after splitting it did not distort the datatypes.
```{r, echo=FALSE}
# remove id, no_pation, and number columns
dataset_of_diabetes_training <- dataset_of_diabetes_training |> 
  select(-id, -no_pation, -number)
str(dataset_of_diabetes_training)
summary(dataset_of_diabetes_training)


# do the same for evaluation data
dataset_of_diabetes_evaluation <- dataset_of_diabetes_evaluation |>
  select(-id, -no_pation, -number)
str(dataset_of_diabetes_evaluation)
summary(dataset_of_diabetes_evaluation)
```

### **Visual Correlation of Variables**
1. To visualize correlations of variables, we use the `corr_cross()` function from the `lares` package, which allows for an easy comparison. This will potentially help us to include potential features when building our model. We set the method to Spearman since most of our data is not normally distributed.
```{r, echo=FALSE, message=FALSE}
#| fig-height: 5
#| fig-width: 10

corr_cross(dataset_of_diabetes_training, 
           method = "spearman", max_pvalue = 0.05, top = 20)
```

# **Data Preparation**
1. Drop vldl column because of potential data entry error, and create new column with vldl computed using tg. Vldl = tg/2.2
2. Replace invalid or non-realistic values with NA
Range:
urea = 1-38.9 mg/dL
cr = 26 - 400 umol/L
hb_a1c = 3.7-14%
chol = 3.0-10.3 mmol/L
tg = keep values mmol/L
hdl = 0.4-4.0 mmol/L
ldl = 0.5-8.0 mm/L
vldl = replace with tg/2.2
bmi = keep

```{r, echo=FALSE}
# for training data
dataset_of_diabetes_training <- dataset_of_diabetes_training |>
  select(-vldl) |>  # Drop existing vldl column
  mutate(
    vldl = tg / 2.2,
    urea = ifelse(urea < 1 | urea > 38.9, NA, urea),
    cr = ifelse(cr < 26 | cr > 400, NA, cr),
    hb_a1c = ifelse(hb_a1c < 3.7 | hb_a1c > 14, NA, hb_a1c),
    chol = ifelse(chol < 3.0 | chol > 10.3, NA, chol),
    hdl = ifelse(hdl < 0.4 | hdl > 4.0, NA, hdl),
    ldl = ifelse(ldl < 0.5 | ldl > 8.0, NA, ldl)
    # tg and bmi are kept as-is
  )
summary(dataset_of_diabetes_training)

# for evaluation data
dataset_of_diabetes_evaluation <- dataset_of_diabetes_evaluation |>
  select(-vldl) |>  # Drop existing vldl column
  mutate(
    vldl = tg / 2.2,
    urea = ifelse(urea < 1 | urea > 38.9, NA, urea),
    cr = ifelse(cr < 26 | cr > 400, NA, cr),
    hb_a1c = ifelse(hb_a1c < 3.7 | hb_a1c > 14, NA, hb_a1c),
    chol = ifelse(chol < 3.0 | chol > 10.3, NA, chol),
    hdl = ifelse(hdl < 0.4 | hdl > 4.0, NA, hdl),
    ldl = ifelse(ldl < 0.5 | ldl > 8.0, NA, ldl)
    # tg and bmi are kept as-is
  )
summary(dataset_of_diabetes_evaluation)
```

###**Imputation of Variables**
```{r, echo=FALSE}
# 1. Remove the target column from both datasets
train_predictors <- dataset_of_diabetes_training[, !(names(dataset_of_diabetes_training) %in% c("target"))]
eval_predictors  <- dataset_of_diabetes_evaluation[, !(names(dataset_of_diabetes_evaluation) %in% c("target"))]

# 2. Combine predictors from training and evaluation sets
combined_predictors <- rbind(train_predictors, eval_predictors)

# 3. Define imputation methods for predictors (make sure the length matches the number of predictors)
method_vector_train <- c(rep("pmm", ncol(combined_predictors)))  # All predictors are imputed with PMM

# 4. Run MICE imputation on the combined predictor data
mice_imp_combined <- mice(combined_predictors, m = 5, method = method_vector_train)

# 5. Complete the imputed dataset (selecting the 3rd imputed version)
complete_data <- complete(mice_imp_combined, 3)

# 6. Reattach the target column after imputation
combined_targets <- c(dataset_of_diabetes_training$target, dataset_of_diabetes_evaluation$target)
complete_data$target <- combined_targets

# 7. Split the imputed dataset back into training and evaluation sets
clean_dataset_of_diabetes_training <- complete_data[1:800, ]
clean_dataset_of_diabetes_evaluation <- complete_data[801:1000, ]

# Plot histograms and QQ plots for the imputed data
plot_histogram(clean_dataset_of_diabetes_training)
plot_qq(clean_dataset_of_diabetes_training)
summary(clean_dataset_of_diabetes_training)

plot_histogram(clean_dataset_of_diabetes_evaluation)
plot_qq(clean_dataset_of_diabetes_evaluation)
summary(clean_dataset_of_diabetes_evaluation)
```

# **Build Models**
### Model 1 - BEST OVERALL
```{r, echo=FALSE, warning=FALSE}

# Fit the multinomial logistic regression model
model1 <- multinom(target ~ gender + hb_a1c + bmi:age + chol, 
                   data = clean_dataset_of_diabetes_training)
cat("Summary Model 1: \n")
summary(model1)

# Get summary of the model (to extract coefficients and standard errors)
summary_model1 <- summary(model1)

# Compute z-scores and p-values
z_scores1 <- summary_model1$coefficients / summary_model1$standard.errors
p_values1 <- 2 * (1 - pnorm(abs(z_scores1)))

# View the p-values
cat("P-values Model 1: \n")
print(p_values1)

cat("Odds Ratio Model 1: \n")
odds_ratios1 <- exp(coef(model1))
print(odds_ratios1)

library(broom)
tidy(model1)
```

### Model 2
```{r, echo=FALSE, warning=FALSE}

# Fit the multinomial logistic regression model
model2 <- multinom(target ~ gender + hb_a1c + bmi:age + chol:tg, 
                   data = clean_dataset_of_diabetes_training)
cat("Summary Model 2: \n")
summary(model2)

# Get summary of the model (to extract coefficients and standard errors)
summary_model2 <- summary(model2)

# Compute z-scores and p-values
z_scores2 <- summary_model2$coefficients / summary_model2$standard.errors
p_values2 <- 2 * (1 - pnorm(abs(z_scores2)))

# View the p-values
cat("P-values Model 2: \n")
print(p_values2)

cat("Odds Ratio Model 2: \n")
odds_ratios2 <- exp(coef(model2))
print(odds_ratios2)

library(broom)
tidy(model2)
```
### Model 3
```{r, echo=FALSE, warning=FALSE}

# Fit the multinomial logistic regression model
model3 <- multinom(target ~ gender + hb_a1c + bmi:age + chol:ldl, 
                   data = clean_dataset_of_diabetes_training)
cat("Summary Model 3: \n")
summary(model3)

# Get summary of the model (to extract coefficients and standard errors)
summary_model3 <- summary(model3)

# Compute z-scores and p-values
z_scores3 <- summary_model3$coefficients / summary_model3$standard.errors
p_values3 <- 2 * (1 - pnorm(abs(z_scores3)))

# View the p-values
cat("P-values Model 3: \n")
print(p_values3)

cat("Odds Ratio Model 3: \n")
odds_ratios3 <- exp(coef(model3))
print(odds_ratios3)

library(broom)
tidy(model3)
```

### Model 4
```{r, echo=FALSE, warning=FALSE}

# Fit the multinomial logistic regression model
model4 <- multinom(target ~ gender + hb_a1c + bmi:age + chol:vldl + cr, 
                   data = clean_dataset_of_diabetes_training)
cat("Summary Model 4: \n")
summary(model4)

# Get summary of the model (to extract coefficients and standard errors)
summary_model4 <- summary(model4)

# Compute z-scores and p-values
z_scores4 <- summary_model4$coefficients / summary_model4$standard.errors
p_values4 <- 2 * (1 - pnorm(abs(z_scores4)))

# View the p-values
cat("P-values Model 4: \n")
print(p_values4)

cat("Odds Ratio Model 4: \n")
odds_ratios4 <- exp(coef(model4))
print(odds_ratios4)

library(broom)
tidy(model4)
```

### Model 5 - Random Forest
```{r, echo=FALSE}
# Automatic Parameter Tuning
tune_grid <- expand.grid(mtry = 1:3)  # Test all possible mtry values

ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = multiClassSummary,
  verboseIter = TRUE
)

# Tune model (takes 2-5 minutes)
rf_tuned <- train(
  target ~ .,
  data = clean_dataset_of_diabetes_training,
  method = "rf",
  metric = "Accuracy",
  tuneGrid = tune_grid,
  trControl = ctrl,
  ntree = 300,  # Smaller for faster tuning
  importance = TRUE
)

# Best parameters
best_mtry <- rf_tuned$bestTune$mtry
cat("Optimal mtry:", best_mtry, "\n")

# --------------------------------------------
# Train Final Model
final_rfmodel <- randomForest(
  target ~ .,
  data = clean_dataset_of_diabetes_training,
  ntree = 500,
  mtry = best_mtry,
  importance = TRUE,
  strata = clean_dataset_of_diabetes_training$target,
  sampsize = rep(min(table(clean_dataset_of_diabetes_training$target)), 3),  # Balanced sampling
  replace = TRUE
)

# --------------------------------------------
# Evaluate
# Predictions
pred_class <- predict(final_rfmodel, dataset_of_diabetes_evaluation)
pred_prob <- predict(final_rfmodel, dataset_of_diabetes_evaluation, type = "prob")

# Confusion Matrix
conf_matrix <- confusionMatrix(pred_class, dataset_of_diabetes_evaluation$target)
print(conf_matrix)

# ROC Curves
par(mfrow = c(1, 3))
for (class in levels(dataset_of_diabetes_evaluation$target)) {
  roc_obj <- roc(
    response = as.numeric(dataset_of_diabetes_evaluation$target == class),
    predictor = pred_prob[, class]
  )
  plot(roc_obj, main = paste("ROC -", class))
  auc_val <- auc(roc_obj)
  legend("bottomright", legend = paste("AUC =", round(auc_val, 2)))
}

# Variable Importance
varImpPlot(final_rfmodel, main = "Predictor Importance")
```


# **Select Models**
### Model 1 (Confusion Matrix, ROC curve, and AUC value) - BEST OVERALL
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10

# Predict class probabilities
prediction_prob_train1 <- predict(model1, clean_dataset_of_diabetes_training, type = "prob")

# Predict most probable class
prediction_class_train1 <- predict(model1, clean_dataset_of_diabetes_training, type = "class")

# Confusion matrix
conf_matrix_train1 <- confusionMatrix(
  factor(prediction_class_train1, levels = c("N", "P", "Y")),
  factor(clean_dataset_of_diabetes_training$target, levels = c("N", "P", "Y"))
)
print(conf_matrix_train1)

# Accuracy
accuracy1 <- mean(prediction_class_train1 == clean_dataset_of_diabetes_training$target)
cat("Accuracy:", accuracy1, "\n")

# Classification error
classification_error_rate1 <- 1 - accuracy1
cat("Classification Error Rate:", classification_error_rate1, "\n")

# ROC and AUC (One-vs-Rest for each class)
true_labels <- clean_dataset_of_diabetes_training$target
classes <- colnames(prediction_prob_train1)

# Plot setup
par(mfrow = c(1, length(classes)))  # one ROC per class

for (class in classes) {
  # Create binary label: 1 if current class, else 0
  binary_response <- ifelse(true_labels == class, 1, 0)
  pred_probs <- prediction_prob_train1[, class]
  
  roc_curve <- roc(binary_response, pred_probs)
  
  # Plot
  plot(roc_curve,
       main = paste("ROC -", class, "\nAUC:", round(auc(roc_curve), 2)),
       col = "red", lwd = 2)
  
  # Print AUC
  cat("AUC for", class, ":", auc(roc_curve), "\n")
}
```

### Model 2
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10

# Predict class probabilities
prediction_prob_train2 <- predict(model2, clean_dataset_of_diabetes_training, type = "prob")

# Predict most probable class
prediction_class_train2 <- predict(model2, clean_dataset_of_diabetes_training, type = "class")

# Confusion matrix
conf_matrix_train2 <- confusionMatrix(
  factor(prediction_class_train1, levels = c("N", "P", "Y")),
  factor(clean_dataset_of_diabetes_training$target, levels = c("N", "P", "Y"))
)
print(conf_matrix_train2)

# Accuracy
accuracy2 <- mean(prediction_class_train2 == clean_dataset_of_diabetes_training$target)
cat("Accuracy:", accuracy2, "\n")

# Classification error
classification_error_rate2 <- 1 - accuracy2
cat("Classification Error Rate:", classification_error_rate2, "\n")

# ROC and AUC (One-vs-Rest for each class)
true_labels <- clean_dataset_of_diabetes_training$target
classes <- colnames(prediction_prob_train2)

# Plot setup
par(mfrow = c(1, length(classes)))  # one ROC per class

for (class in classes) {
  # Create binary label: 1 if current class, else 0
  binary_response <- ifelse(true_labels == class, 1, 0)
  pred_probs <- prediction_prob_train1[, class]
  
  roc_curve <- roc(binary_response, pred_probs)
  
  # Plot
  plot(roc_curve,
       main = paste("ROC -", class, "\nAUC:", round(auc(roc_curve), 2)),
       col = "red", lwd = 2)
  
  # Print AUC
  cat("AUC for", class, ":", auc(roc_curve), "\n")
}
```

### Model 3
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10

# Predict class probabilities
prediction_prob_train3 <- predict(model3, clean_dataset_of_diabetes_training, type = "prob")

# Predict most probable class
prediction_class_train3 <- predict(model3, clean_dataset_of_diabetes_training, type = "class")

# Confusion matrix
conf_matrix_train3 <- confusionMatrix(
  factor(prediction_class_train3, levels = c("N", "P", "Y")),
  factor(clean_dataset_of_diabetes_training$target, levels = c("N", "P", "Y"))
)
print(conf_matrix_train3)

# Accuracy
accuracy3 <- mean(prediction_class_train3 == clean_dataset_of_diabetes_training$target)
cat("Accuracy:", accuracy3, "\n")

# Classification error
classification_error_rate3 <- 1 - accuracy3
cat("Classification Error Rate:", classification_error_rate3, "\n")

# ROC and AUC (One-vs-Rest for each class)
true_labels <- clean_dataset_of_diabetes_training$target
classes <- colnames(prediction_prob_train3)

# Plot setup
par(mfrow = c(1, length(classes)))  # one ROC per class

for (class in classes) {
  # Create binary label: 1 if current class, else 0
  binary_response <- ifelse(true_labels == class, 1, 0)
  pred_probs <- prediction_prob_train1[, class]
  
  roc_curve <- roc(binary_response, pred_probs)
  
  # Plot
  plot(roc_curve,
       main = paste("ROC -", class, "\nAUC:", round(auc(roc_curve), 2)),
       col = "red", lwd = 2)
  
  # Print AUC
  cat("AUC for", class, ":", auc(roc_curve), "\n")
}
```

### Model 4
```{r, echo=FALSE}
#| fig-height: 5
#| fig-width: 10

# Predict class probabilities
prediction_prob_train4 <- predict(model4, clean_dataset_of_diabetes_training, type = "prob")

# Predict most probable class
prediction_class_train4 <- predict(model4, clean_dataset_of_diabetes_training, type = "class")

# Confusion matrix
conf_matrix_train4 <- confusionMatrix(
  factor(prediction_class_train1, levels = c("N", "P", "Y")),
  factor(clean_dataset_of_diabetes_training$target, levels = c("N", "P", "Y"))
)
print(conf_matrix_train4)

# Accuracy
accuracy4 <- mean(prediction_class_train4 == clean_dataset_of_diabetes_training$target)
cat("Accuracy:", accuracy4, "\n")

# Classification error
classification_error_rate4 <- 1 - accuracy4
cat("Classification Error Rate:", classification_error_rate1, "\n")

# ROC and AUC (One-vs-Rest for each class)
true_labels <- clean_dataset_of_diabetes_training$target
classes <- colnames(prediction_prob_train1)

# Plot setup
par(mfrow = c(1, length(classes)))  # one ROC per class

for (class in classes) {
  # Create binary label: 1 if current class, else 0
  binary_response <- ifelse(true_labels == class, 1, 0)
  pred_probs <- prediction_prob_train1[, class]
  
  roc_curve <- roc(binary_response, pred_probs)
  
  # Plot
  plot(roc_curve,
       main = paste("ROC -", class, "\nAUC:", round(auc(roc_curve), 2)),
       col = "red", lwd = 2)
  
  # Print AUC
  cat("AUC for", class, ":", auc(roc_curve), "\n")
}
```

### Model 5 - Random Forest
```{r, echo=FALSE}
# Predict on evaluation data
prediction_prob_eval <- predict(final_rfmodel, clean_dataset_of_diabetes_evaluation, type = "prob")
prediction_class_eval <- predict(final_rfmodel, clean_dataset_of_diabetes_evaluation, type = "class")

# Confusion Matrix
conf_matrix_eval <- confusionMatrix(
  factor(prediction_class_eval, levels = c("N", "P", "Y")),
  factor(clean_dataset_of_diabetes_evaluation$target, levels = c("N", "P", "Y"))
)
print(conf_matrix_eval)

# Accuracy
accuracy_eval <- mean(prediction_class_eval == clean_dataset_of_diabetes_evaluation$target)
cat("Accuracy on Evaluation Data:", accuracy_eval, "\n")

# ROC and AUC for each class
true_labels_eval <- clean_dataset_of_diabetes_evaluation$target
classes_eval <- colnames(prediction_prob_eval)

par(mfrow = c(1, length(classes_eval)))  # one ROC per class
for (class in classes_eval) {
  binary_response_eval <- ifelse(true_labels_eval == class, 1, 0)
  pred_probs_eval <- prediction_prob_eval[, class]
  
  roc_curve_eval <- roc(binary_response_eval, pred_probs_eval)
  
  plot(roc_curve_eval,
       main = paste("ROC (Evaluation) -", class, "\nAUC:", round(auc(roc_curve_eval), 2)),
       col = "blue", lwd = 2)
  
  cat("AUC for", class, ":", auc(roc_curve_eval), "\n")
}
```

# **Predictions on Evaluation Dataset**

```{r, echo=FALSE}
# Drop the target column from the evaluation dataset before prediction
eval_data_no_target <- clean_dataset_of_diabetes_evaluation[, setdiff(names(clean_dataset_of_diabetes_evaluation), "target")]

# Predict on evaluation data (no target column)
prediction_prob_eval <- predict(final_rfmodel, eval_data_no_target, type = "prob")
prediction_class_eval <- predict(final_rfmodel, eval_data_no_target, type = "class")

# Confusion Matrix
conf_matrix_eval <- confusionMatrix(
  factor(prediction_class_eval, levels = c("N", "P", "Y")),
  factor(clean_dataset_of_diabetes_evaluation$target, levels = c("N", "P", "Y"))
)
print(conf_matrix_eval)

# Accuracy
accuracy_eval <- mean(prediction_class_eval == clean_dataset_of_diabetes_evaluation$target)
cat("Accuracy on Evaluation Data:", accuracy_eval, "\n")

# ROC and AUC for each class
true_labels_eval <- clean_dataset_of_diabetes_evaluation$target
classes_eval <- colnames(prediction_prob_eval)

par(mfrow = c(1, length(classes_eval)))  # one ROC per class
for (class in classes_eval) {
  binary_response_eval <- ifelse(true_labels_eval == class, 1, 0)
  pred_probs_eval <- prediction_prob_eval[, class]
  
  roc_curve_eval <- roc(binary_response_eval, pred_probs_eval)
  
  plot(roc_curve_eval,
       main = paste("ROC (Evaluation) -", class, "\nAUC:", round(auc(roc_curve_eval), 2)),
       col = "blue", lwd = 2)
  
  cat("AUC for", class, ":", auc(roc_curve_eval), "\n")
}

head(prediction_class_eval, 20)
```

# **Comparison of Actual Target vs. Predicted Target From Using Model 1**

```{r, echo=FALSE}
# Create a comparison data frame
comparison_df <- data.frame(
  Actual = clean_dataset_of_diabetes_evaluation$target,
  Predicted = prediction_class_eval
)

# View first few rows of the comparison
head(comparison_df, 50)

# Optional: View mismatches only
mismatches <- comparison_df[comparison_df$Actual != comparison_df$Predicted, ]
head(mismatches, 50)
```

# **Comparing Models Performance**

```{r}
# Function to extract metrics from a model
extract_metrics <- function(model, data, model_name, is_rf = FALSE) {
  if (is_rf) {
    pred_prob <- predict(model, data, type = "prob")
    pred_class <- predict(model, data, type = "class")
  } else {
    pred_prob <- predict(model, data, type = "prob")
    pred_class <- predict(model, data, type = "class")
  }
  
  conf_matrix <- confusionMatrix(
    factor(pred_class, levels = c("N", "P", "Y")),
    factor(data$target, levels = c("N", "P", "Y"))
  )
  
  metrics <- data.frame(
    Model = model_name,
    Accuracy = conf_matrix$overall["Accuracy"],
    Kappa = conf_matrix$overall["Kappa"],
    Sensitivity_N = conf_matrix$byClass["Class: N", "Sensitivity"],
    Specificity_N = conf_matrix$byClass["Class: N", "Specificity"],
    Sensitivity_P = conf_matrix$byClass["Class: P", "Sensitivity"],
    Specificity_P = conf_matrix$byClass["Class: P", "Specificity"],
    Sensitivity_Y = conf_matrix$byClass["Class: Y", "Sensitivity"],
    Specificity_Y = conf_matrix$byClass["Class: Y", "Specificity"]
  )
  
  # Calculate AUC for each class
  true_labels <- data$target
  classes <- if(is_rf) colnames(pred_prob) else colnames(pred_prob)
  
  for (class in classes) {
    binary_response <- ifelse(true_labels == class, 1, 0)
    pred_probs <- if(is_rf) pred_prob[, class] else pred_prob[, class]
    roc_curve <- roc(binary_response, pred_probs)
    metrics[[paste0("AUC_", class)]] <- auc(roc_curve)
  }
  
  return(metrics)
}

# Create training metrics table
training_metrics <- rbind(
  extract_metrics(model1, clean_dataset_of_diabetes_training, "Model 1 (Multinomial)"),
  extract_metrics(model2, clean_dataset_of_diabetes_training, "Model 2 (Multinomial)"),
  extract_metrics(model3, clean_dataset_of_diabetes_training, "Model 3 (Multinomial)"),
  extract_metrics(model4, clean_dataset_of_diabetes_training, "Model 4 (Multinomial)"),
  extract_metrics(final_rfmodel, clean_dataset_of_diabetes_training, "Model 5 (Random Forest)", is_rf = TRUE)
)

# Create evaluation metrics table
evaluation_metrics <- rbind(
  extract_metrics(model1, clean_dataset_of_diabetes_evaluation, "Model 1 (Multinomial)"),
  extract_metrics(model2, clean_dataset_of_diabetes_evaluation, "Model 2 (Multinomial)"),
  extract_metrics(model3, clean_dataset_of_diabetes_evaluation, "Model 3 (Multinomial)"),
  extract_metrics(model4, clean_dataset_of_diabetes_evaluation, "Model 4 (Multinomial)"),
  extract_metrics(final_rfmodel, clean_dataset_of_diabetes_evaluation, "Model 5 (Random Forest)", is_rf = TRUE)
)

# Display the tables with better formatting

cat("\n\n### Training Data Performance Metrics\n\n")
kable(training_metrics, digits = 3, 
      caption = "Model Performance on Training Data") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

cat("\n\n### Evaluation Data Performance Metrics\n\n")
kable(evaluation_metrics, digits = 3, 
      caption = "Model Performance on Evaluation Data") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
